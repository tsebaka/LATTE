{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b1b19ca-c154-46fd-a303-f80b82d3512c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def _safe_div(a, b):\n",
    "    return np.where(b != 0, a / b, 0.0)\n",
    "\n",
    "def entropy_from_counts(cnts: np.ndarray) -> float:\n",
    "    total = cnts.sum()\n",
    "    if total <= 0:\n",
    "        return 0.0\n",
    "    p = cnts / total\n",
    "    p = p[p > 0]\n",
    "    return float(-(p * np.log(p)).sum())\n",
    "\n",
    "def hhi_from_counts(cnts: np.ndarray) -> float:\n",
    "    total = cnts.sum()\n",
    "    if total <= 0:\n",
    "        return 0.0\n",
    "    p = cnts / total\n",
    "    return float((p * p).sum())\n",
    "\n",
    "def gini(x: np.ndarray) -> float:\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    if x.size == 0 or np.all(x == 0):\n",
    "        return 0.0\n",
    "    x = np.sort(np.abs(x))\n",
    "    n = x.size\n",
    "    cumx = np.cumsum(x)\n",
    "    return float((n + 1 - 2 * (cumx.sum() / cumx[-1])) / n)\n",
    "\n",
    "def as_float32(df):\n",
    "    for c in df.columns:\n",
    "        if pd.api.types.is_float_dtype(df[c]):\n",
    "            df[c] = df[c].astype(np.float32)\n",
    "        elif pd.api.types.is_integer_dtype(df[c]) and c != 'cl_id':\n",
    "            df[c] = df[c].astype(np.int32)\n",
    "    return df\n",
    "\n",
    "_MONTHS = {'JAN':1,'FEB':2,'MAR':3,'APR':4,'MAY':5,'JUN':6,'JUL':7,'AUG':8,'SEP':9,'OCT':10,'NOV':11,'DEC':12}\n",
    "_trxdt_re = re.compile(r'(?i)^\\s*(\\d{1,2})([A-Z]{3})(\\d{2})(?::(\\d{2}))?(?::(\\d{2}))?(?::(\\d{2}))?\\s*$')\n",
    "\n",
    "def parse_trdatetime(s: str):\n",
    "    if pd.isna(s):\n",
    "        return pd.NaT\n",
    "    s = str(s).strip()\n",
    "    m = _trxdt_re.match(s)\n",
    "    if m:\n",
    "        d, mon, yy, hh, mm, ss = m.groups()\n",
    "        d = int(d); mon = _MONTHS.get(mon.upper(), 1); yy = int(yy)\n",
    "        year = 2000 + yy if yy <= 69 else 1900 + yy\n",
    "        hh = int(hh) if hh else 0; mm = int(mm) if mm else 0; ss = int(ss) if ss else 0\n",
    "        try:\n",
    "            return datetime(year, mon, d, hh, mm, ss)\n",
    "        except ValueError:\n",
    "            return pd.NaT\n",
    "    return pd.to_datetime(s, errors='coerce', dayfirst=True, utc=False)\n",
    "\n",
    "def _days_from_origin(dt_series: pd.Series, origin):\n",
    "    origin = pd.to_datetime(origin)\n",
    "    return (pd.to_datetime(dt_series) - origin).dt.days.astype('Int64')\n",
    "\n",
    "INCOME_CATS = {'DEPOSIT','SALARY','C2C_IN','C2A_IN','P2P_IN','REVERSAL_IN','REFUND','CREDITS_IN'}\n",
    "EXPENSE_CATS = {'POS','CASH','WD_ATM_ROS','WD_ATM_IN','WD_ATM','C2C_OUT','C2A_OUT','P2P_OUT','COMMISSIONS','FEES'}\n",
    "\n",
    "\n",
    "class FeatureFactoryRosbank:\n",
    "    def __init__(\n",
    "        self,\n",
    "        topk_mcc: int = 300,\n",
    "        topk_trxcat: int = 50,\n",
    "        topk_channel: int = 10,\n",
    "        tfidf_max_features_mcc: int = 800,\n",
    "        tfidf_max_features_trx: int = 300,\n",
    "        use_dense_tfidf: bool = True,\n",
    "        fixed_origin: str | None = None,\n",
    "    ):\n",
    "        self.topk_mcc = topk_mcc\n",
    "        self.topk_trxcat = topk_trxcat\n",
    "        self.topk_channel = topk_channel\n",
    "        self.tfidf_max_features_mcc = tfidf_max_features_mcc\n",
    "        self.tfidf_max_features_trx = tfidf_max_features_trx\n",
    "        self.use_dense_tfidf = use_dense_tfidf\n",
    "        self.fixed_origin = fixed_origin\n",
    "\n",
    "        self.origin_ = None\n",
    "        self.channel_map_ = None\n",
    "        self.trxcat_map_ = None\n",
    "        self.top_mcc_ = None\n",
    "        self.top_trx_ = None\n",
    "        self.top_chan_ = None\n",
    "        self.tfidf_mcc_ = None\n",
    "        self.tfidf_trx_ = None\n",
    "\n",
    "    def _preprocess(self, df: pd.DataFrame, is_fit: bool):\n",
    "        need_cols = {'cl_id','MCC','channel_type','currency','TRDATETIME','amount','trx_category'}\n",
    "        missing = need_cols - set(df.columns)\n",
    "        if missing:\n",
    "            raise KeyError(f\"Отсутствуют колонки: {missing}\")\n",
    "\n",
    "        d = df.copy()\n",
    "\n",
    "        trdt = d['TRDATETIME'].apply(parse_trdatetime)\n",
    "        d['trx_ts'] = pd.to_datetime(trdt)\n",
    "        d = d[~d['trx_ts'].isna()].reset_index(drop=True)\n",
    "\n",
    "        if is_fit:\n",
    "            self.origin_ = pd.to_datetime(self.fixed_origin) if self.fixed_origin else pd.to_datetime(d['trx_ts'].min())\n",
    "\n",
    "        d['trans_date'] = _days_from_origin(d['trx_ts'], self.origin_).fillna(0).astype(np.int32)\n",
    "\n",
    "        d['cl_id']   = pd.to_numeric(d['cl_id'], errors='coerce').fillna(-1).astype(np.int32)\n",
    "        d['MCC']     = pd.to_numeric(d['MCC'], errors='coerce').fillna(-1).astype(np.int32)\n",
    "        d['currency']= pd.to_numeric(d['currency'], errors='coerce').fillna(-1).astype(np.int32)\n",
    "        d['amount']  = pd.to_numeric(d['amount'], errors='coerce').fillna(0.0).astype(np.float32)\n",
    "\n",
    "        if is_fit:\n",
    "            chan_uniques = pd.Series(d['channel_type'].astype(str).fillna('unknown')).unique().tolist()\n",
    "            trx_uniques  = pd.Series(d['trx_category'].astype(str).fillna('unknown')).unique().tolist()\n",
    "            self.channel_map_ = {cat: i for i, cat in enumerate(chan_uniques)}\n",
    "            self.trxcat_map_  = {cat: i for i, cat in enumerate(trx_uniques)}\n",
    "\n",
    "        d['channel_code'] = pd.Series([self.channel_map_.get(str(x), -1) for x in d['channel_type']]).astype(np.int32)\n",
    "        d['trxcat_code']  = pd.Series([self.trxcat_map_.get(str(x), -1) for x in d['trx_category']]).astype(np.int32)\n",
    "\n",
    "        trx_upper = d['trx_category'].astype(str).str.upper()\n",
    "        d['is_income']  = trx_upper.isin(INCOME_CATS).astype(np.int8)\n",
    "        d['is_expense'] = trx_upper.isin(EXPENSE_CATS).astype(np.int8)\n",
    "        d['amount_income']  = d['amount'] * d['is_income']\n",
    "        d['amount_expense'] = d['amount'] * d['is_expense']\n",
    "\n",
    "        return d\n",
    "\n",
    "    def fit(self, df: pd.DataFrame):\n",
    "        d = self._preprocess(df, is_fit=True)\n",
    "\n",
    "        self.top_mcc_  = d['MCC'].value_counts().head(self.topk_mcc).index.tolist()\n",
    "        self.top_trx_  = d['trxcat_code'].value_counts().head(self.topk_trxcat).index.tolist()\n",
    "        self.top_chan_ = d['channel_code'].value_counts().head(self.topk_channel).index.tolist()\n",
    "\n",
    "        seq_mcc = (\n",
    "            d.sort_values(['cl_id','trans_date'])\n",
    "             .groupby('cl_id')['MCC']\n",
    "             .apply(lambda s: ' '.join(s.astype(str)))\n",
    "        )\n",
    "        self.tfidf_mcc_ = TfidfVectorizer(\n",
    "            max_features=self.tfidf_max_features_mcc,\n",
    "            dtype=np.float32,\n",
    "            token_pattern=r'(?u)\\b\\w+\\b'\n",
    "        )\n",
    "        self.tfidf_mcc_.fit(seq_mcc.values)\n",
    "\n",
    "        seq_trx = (\n",
    "            d.sort_values(['cl_id','trans_date'])\n",
    "             .groupby('cl_id')['trxcat_code']\n",
    "             .apply(lambda s: ' '.join(s.astype(str)))\n",
    "        )\n",
    "        self.tfidf_trx_ = TfidfVectorizer(\n",
    "            max_features=self.tfidf_max_features_trx,\n",
    "            dtype=np.float32,\n",
    "            token_pattern=r'(?u)\\b\\w+\\b'\n",
    "        )\n",
    "        self.tfidf_trx_.fit(seq_trx.values)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        if any(x is None for x in [self.origin_, self.channel_map_, self.trxcat_map_, self.top_mcc_, self.top_trx_, self.top_chan_, self.tfidf_mcc_, self.tfidf_trx_]):\n",
    "            raise RuntimeError(\".fit() firstly\")\n",
    "\n",
    "        d = self._preprocess(df, is_fit=False)\n",
    "\n",
    "        agg = d.groupby('cl_id').agg(\n",
    "            tx_cnt=('amount','size'),\n",
    "            sum_amount=('amount','sum'),\n",
    "            mean_amount=('amount','mean'),\n",
    "            std_amount=('amount','std'),\n",
    "            min_amount=('amount','min'),\n",
    "            max_amount=('amount','max'),\n",
    "            q25_amount=('amount', lambda x: np.percentile(x, 25)),\n",
    "            q50_amount=('amount', lambda x: np.percentile(x, 50)),\n",
    "            q75_amount=('amount', lambda x: np.percentile(x, 75)),\n",
    "            sum_income=('amount_income','sum'),\n",
    "            sum_expense=('amount_expense','sum'),\n",
    "            cnt_income=('is_income','sum'),\n",
    "            cnt_expense=('is_expense','sum'),\n",
    "            first_day=('trans_date','min'),\n",
    "            last_day=('trans_date','max'),\n",
    "            active_days=('trans_date','nunique'),\n",
    "            uniq_mcc=('MCC','nunique'),\n",
    "            uniq_trxcat=('trxcat_code','nunique'),\n",
    "            uniq_channel=('channel_code','nunique'),\n",
    "            uniq_currency=('currency','nunique'),\n",
    "        ).reset_index()\n",
    "\n",
    "        agg['tx_cnt'] = agg['tx_cnt'].astype(np.int32)\n",
    "        agg['period'] = (agg['last_day'] - agg['first_day'] + 1).astype(np.int32)\n",
    "        agg['tx_per_day'] = _safe_div(agg['tx_cnt'].astype(np.float32), agg['period'].astype(np.float32))\n",
    "        agg['days_ratio'] = _safe_div(agg['active_days'].astype(np.float32), agg['period'].astype(np.float32))\n",
    "        agg['income_share']  = _safe_div(agg['sum_income'], agg['sum_amount'] + 1e-9)\n",
    "        agg['expense_share'] = _safe_div(agg['sum_expense'], agg['sum_amount'] + 1e-9)\n",
    "\n",
    "        day_agg = (\n",
    "            d.groupby(['cl_id','trans_date'])\n",
    "             .agg(day_sum=('amount','sum'),\n",
    "                  day_cnt=('amount','size'),\n",
    "                  day_income=('amount_income','sum'),\n",
    "                  day_expense=('amount_expense','sum'))\n",
    "             .reset_index()\n",
    "        )\n",
    "        daily = day_agg.groupby('cl_id').agg(\n",
    "            day_sum_mean=('day_sum','mean'),\n",
    "            day_sum_std =('day_sum','std'),\n",
    "            day_cnt_mean=('day_cnt','mean'),\n",
    "            day_cnt_std =('day_cnt','std'),\n",
    "            day_sum_q90 =('day_sum', lambda x: np.percentile(x, 90)),\n",
    "            day_income_mean=('day_income','mean'),\n",
    "            day_expense_mean=('day_expense','mean'),\n",
    "        ).reset_index()\n",
    "\n",
    "        def _gap_stats(s):\n",
    "            v = np.sort(s.values)\n",
    "            if v.size <= 1:\n",
    "                return pd.Series(dict(gap_mean=0.0, gap_median=0.0))\n",
    "            gaps = np.diff(v)\n",
    "            return pd.Series(dict(gap_mean=float(gaps.mean()), gap_median=float(np.median(gaps))))\n",
    "        gaps = d.groupby('cl_id')['trans_date'].apply(_gap_stats).reset_index()\n",
    "        gaps = gaps.pivot(index='cl_id', columns='level_1', values='trans_date').reset_index()\n",
    "\n",
    "        def _dist_stats(df_g, col, prefix):\n",
    "            cnt = df_g.groupby(['cl_id', col]).size().rename('cnt').reset_index()\n",
    "            g = cnt.groupby('cl_id')['cnt']\n",
    "            stats = pd.DataFrame({\n",
    "                f'{prefix}_entropy': g.apply(lambda s: entropy_from_counts(s.to_numpy())),\n",
    "                f'{prefix}_hhi':     g.apply(lambda s: hhi_from_counts(s.to_numpy())),\n",
    "                f'{prefix}_top1_share': g.apply(lambda s: float(s.max() / s.sum()) if s.sum() > 0 else 0.0),\n",
    "            }).reset_index()\n",
    "            return stats\n",
    "\n",
    "        mcc_stats   = _dist_stats(d, 'MCC', 'mcc')\n",
    "        trxcat_stats= _dist_stats(d, 'trxcat_code', 'trxcat')\n",
    "        chan_stats  = _dist_stats(d, 'channel_code', 'chan')\n",
    "\n",
    "        def _top_pivots_fixed(df_g, col, top_vals, prefix, val_col='amount'):\n",
    "            mask = df_g[col].isin(top_vals)\n",
    "            p_cnt = df_g[mask].groupby(['cl_id', col]).size().unstack(fill_value=0)\n",
    "            p_cnt = p_cnt.reindex(columns=top_vals, fill_value=0).add_prefix(f'{prefix}cnt_').reset_index()\n",
    "\n",
    "            p_sum = df_g[mask].groupby(['cl_id', col])[val_col].sum().unstack(fill_value=0.0)\n",
    "            p_sum = p_sum.reindex(columns=top_vals, fill_value=0.0).add_prefix(f'{prefix}sum_').reset_index()\n",
    "\n",
    "            p_mean = df_g[mask].groupby(['cl_id', col])[val_col].mean().unstack()\n",
    "            p_mean = p_mean.reindex(columns=top_vals).fillna(0.0).add_prefix(f'{prefix}mean_').reset_index()\n",
    "            return p_cnt, p_sum, p_mean\n",
    "\n",
    "        mcc_cnt, mcc_sum, mcc_mean    = _top_pivots_fixed(d, 'MCC',          self.top_mcc_,  'mcc_')\n",
    "        trx_cnt, trx_sum, trx_mean    = _top_pivots_fixed(d, 'trxcat_code',  self.top_trx_,  'trx_')\n",
    "        chan_cnt, chan_sum, chan_mean = _top_pivots_fixed(d, 'channel_code', self.top_chan_, 'chan_')\n",
    "\n",
    "        seq_mcc = (\n",
    "            d.sort_values(['cl_id','trans_date'])\n",
    "             .groupby('cl_id')['MCC']\n",
    "             .apply(lambda s: ' '.join(s.astype(str)))\n",
    "        )\n",
    "        X_mcc = self.tfidf_mcc_.transform(seq_mcc.values)\n",
    "        mcc_cols = [f'tfidf_mcc_{t}' for t in self.tfidf_mcc_.get_feature_names_out()]\n",
    "        mcc_df = pd.DataFrame(X_mcc.toarray(), columns=mcc_cols)\n",
    "        mcc_df.insert(0, 'cl_id', seq_mcc.index.values.astype(np.int32))\n",
    "\n",
    "        seq_trx = (\n",
    "            d.sort_values(['cl_id','trans_date'])\n",
    "             .groupby('cl_id')['trxcat_code']\n",
    "             .apply(lambda s: ' '.join(s.astype(str)))\n",
    "        )\n",
    "        X_trx = self.tfidf_trx_.transform(seq_trx.values)\n",
    "        trx_cols = [f'tfidf_trx_{t}' for t in self.tfidf_trx_.get_feature_names_out()]\n",
    "        trx_df = pd.DataFrame(X_trx.toarray(), columns=trx_cols)\n",
    "        trx_df.insert(0, 'cl_id', seq_trx.index.values.astype(np.int32))\n",
    "\n",
    "        gini_series = (\n",
    "            d.groupby('cl_id')['amount'].apply(lambda s: gini(s.to_numpy())).astype(np.float32)\n",
    "        ).rename('amount_gini')\n",
    "\n",
    "        feats = (agg\n",
    "                 .merge(daily, on='cl_id', how='left')\n",
    "                 .merge(gaps, on='cl_id', how='left')\n",
    "                 .merge(mcc_stats, on='cl_id', how='left')\n",
    "                 .merge(trxcat_stats, on='cl_id', how='left')\n",
    "                 .merge(chan_stats, on='cl_id', how='left')\n",
    "                 .merge(mcc_cnt, on='cl_id', how='left')\n",
    "                 .merge(mcc_sum, on='cl_id', how='left')\n",
    "                 .merge(mcc_mean, on='cl_id', how='left')\n",
    "                 .merge(trx_cnt, on='cl_id', how='left')\n",
    "                 .merge(trx_sum, on='cl_id', how='left')\n",
    "                 .merge(trx_mean, on='cl_id', how='left')\n",
    "                 .merge(chan_cnt, on='cl_id', how='left')\n",
    "                 .merge(chan_sum, on='cl_id', how='left')\n",
    "                 .merge(chan_mean, on='cl_id', how='left')\n",
    "                 .merge(mcc_df, on='cl_id', how='left')\n",
    "                 .merge(trx_df, on='cl_id', how='left')\n",
    "                 .merge(gini_series.reset_index(), on='cl_id', how='left'))\n",
    "\n",
    "        feats = feats.replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
    "        feats = as_float32(feats)\n",
    "        feats.attrs['origin_datetime'] = pd.Timestamp(self.origin_)\n",
    "        return feats\n",
    "\n",
    "    def save(self, path: str):\n",
    "        data = dict(\n",
    "            origin_=self.origin_,\n",
    "            channel_map_=self.channel_map_,\n",
    "            trxcat_map_=self.trxcat_map_,\n",
    "            top_mcc_=self.top_mcc_,\n",
    "            top_trx_=self.top_trx_,\n",
    "            top_chan_=self.top_chan_,\n",
    "            tfidf_mcc_=self.tfidf_mcc_,\n",
    "            tfidf_trx_=self.tfidf_trx_,\n",
    "            params=dict(\n",
    "                topk_mcc=self.topk_mcc,\n",
    "                topk_trxcat=self.topk_trxcat,\n",
    "                topk_channel=self.topk_channel,\n",
    "                tfidf_max_features_mcc=self.tfidf_max_features_mcc,\n",
    "                tfidf_max_features_trx=self.tfidf_max_features_trx,\n",
    "                use_dense_tfidf=self.use_dense_tfidf,\n",
    "                fixed_origin=self.fixed_origin,\n",
    "            ),\n",
    "        )\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: str) -> \"FeatureFactoryRosbank\":\n",
    "        with open(path, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        obj = cls(**data[\"params\"])\n",
    "        obj.origin_ = data[\"origin_\"]\n",
    "        obj.channel_map_ = data[\"channel_map_\"]\n",
    "        obj.trxcat_map_ = data[\"trxcat_map_\"]\n",
    "        obj.top_mcc_ = data[\"top_mcc_\"]\n",
    "        obj.top_trx_ = data[\"top_trx_\"]\n",
    "        obj.top_chan_ = data[\"top_chan_\"]\n",
    "        obj.tfidf_mcc_ = data[\"tfidf_mcc_\"]\n",
    "        obj.tfidf_trx_ = data[\"tfidf_trx_\"]\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30651dfb-b22f-4382-beca-c757bbbcc0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/home/jovyan/zoloev-madvillainy/LATTE/data/rosbank/train.csv')\n",
    "test_ids = pd.read_csv('/home/jovyan/zoloev-madvillainy/LATTE/data/rosbank/test_ids.csv')\n",
    "\n",
    "train_df = train[~train[\"cl_id\"].isin(set(test_ids[\"cl_id\"]))].reset_index(drop=True)\n",
    "test_df = train[train[\"cl_id\"].isin(set(test_ids[\"cl_id\"]))].reset_index(drop=True)\n",
    "\n",
    "ff = FeatureFactoryRosbank(topk_mcc=400, topk_trxcat=80)\n",
    "ff.fit(train_df)\n",
    "X_train = ff.transform(train_df.drop(columns=['target_flag', 'target_sum']))\n",
    "X_test  = ff.transform(test_df.drop(columns=['target_flag', 'target_sum']))\n",
    "\n",
    "soreva_test = pd.read_csv('/home/jovyan/zoloev-madvillainy/LATTE/data/rosbank/test.csv')\n",
    "X_soreva_test  = ff.transform(soreva_test)\n",
    "\n",
    "train_test = pd.concat((X_train, X_test, X_soreva_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e847764-bab3-415b-a2de-2c568e20abd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "labels = (\n",
    "    train_df.groupby(\"cl_id\")[\"target_flag\"]\n",
    "    .max()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "X_train = X_train.merge(labels, on='cl_id')\n",
    "y_train = X_train['target_flag']\n",
    "\n",
    "model = lgb.LGBMClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.02,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "model.fit(X_train.drop(columns=['target_flag', 'cl_id']), y_train)\n",
    "\n",
    "importances = pd.Series(model.feature_importances_, index=X_train.drop(columns=['target_flag', 'cl_id']).columns)\n",
    "top_features = importances.sort_values(ascending=False).head(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "340cf582-38a0-4312-914b-7fb8c9d662ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final = train_test[list(top_features.index) + ['cl_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8e86e6-0027-44d4-968a-05c07d6f4847",
   "metadata": {},
   "source": [
    "# embs and descr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79ae90fc-d496-4778-ae8a-382c254400c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = pd.read_parquet('data/rosbank/embeddings/rosbank_base.parquet')\n",
    "statisctics_and_descriptions = Final.merge(descriptions, on='cl_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8184b47c-d341-4ddc-b87d-b7a873a96706",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "labels = (\n",
    "    train_df.groupby(\"cl_id\")[\"target_flag\"]\n",
    "    .max()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "X_train = statisctics_and_descriptions.merge(labels, on='cl_id')\n",
    "y_train = X_train['target_flag']\n",
    "\n",
    "model = lgb.LGBMClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.02,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "model.fit(X_train.drop(columns=['target_flag', 'cl_id']), y_train)\n",
    "\n",
    "importances = pd.Series(model.feature_importances_, index=X_train.drop(columns=['target_flag', 'cl_id']).columns)\n",
    "top_features = importances.sort_values(ascending=False).head(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c450392e-e76b-4584-b5f8-21f60d25cc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final = statisctics_and_descriptions[list(top_features.index) + ['cl_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01669caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "id_col = \"cl_id\"\n",
    "\n",
    "ids_test = set(labels[id_col].values)\n",
    "\n",
    "X_train_df = Final[~Final[id_col].isin(ids_test)].reset_index(drop=True)\n",
    "X_test_df  = Final[ Final[id_col].isin(ids_test)].reset_index(drop=True)\n",
    "\n",
    "feat_cols = [c for c in Final.columns if c != id_col]\n",
    "\n",
    "X_train = X_train_df[feat_cols].astype(np.float32).values\n",
    "X_test  = X_test_df[feat_cols].astype(np.float32).values\n",
    "\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "scaler.fit(X_train)\n",
    "\n",
    "Z_train = scaler.transform(X_train).astype(np.float32)\n",
    "Z_test  = scaler.transform(X_test).astype(np.float32)\n",
    "\n",
    "train_embs = pd.DataFrame({\n",
    "    id_col: X_train_df[id_col].astype(np.int32).values,\n",
    "    \"embs\": list(Z_train),\n",
    "})\n",
    "test_embs = pd.DataFrame({\n",
    "    id_col: X_test_df[id_col].astype(np.int32).values,\n",
    "    \"embs\": list(Z_test),\n",
    "})\n",
    "\n",
    "scaled_final = pd.concat((train_embs, test_embs))\n",
    "\n",
    "scaled_final.to_csv(\n",
    "    'data/rosbank/embeddings/scaled_embs_and_descr.csv',\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1ce174",
   "metadata": {},
   "outputs": [],
   "source": [
    "latte_s = pd.read_pickle('data/rosbank/latte-s/con_embs_1.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2163cf5e-a609-4609-9a76-fe55258e0178",
   "metadata": {},
   "outputs": [],
   "source": [
    "latte_s['cl_id'] = latte_s['cl_id'].astype(int)\n",
    "Final['cl_id'] = Final['cl_id'].astype(int)\n",
    "\n",
    "final_latte_s = Final.merge(latte_s, on='cl_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a94e3dd-e8a3-4e7e-a1dd-b892c79fe3e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "labels = (\n",
    "    train_df.groupby(\"cl_id\")[\"target_flag\"]\n",
    "    .max()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "X_train = final_latte_s.merge(labels, on='cl_id')\n",
    "y_train = X_train['target_flag']\n",
    "\n",
    "model = lgb.LGBMClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.02,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "model.fit(X_train.drop(columns=['target_flag', 'cl_id']), y_train)\n",
    "\n",
    "importances = pd.Series(model.feature_importances_, index=X_train.drop(columns=['target_flag', 'cl_id']).columns)\n",
    "top_features = importances.sort_values(ascending=False).head(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d045ba3-9c02-4464-87ee-97cba9a4695e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final = final_latte_s[list(top_features.index) + ['cl_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d58176-5788-4a2f-aa86-1b48a517418b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc85550b-4479-42d7-a79e-d1469837fb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "id_col = \"cl_id\"\n",
    "\n",
    "ids_test = set(labels[id_col].values)\n",
    "\n",
    "X_train_df = Final[~Final[id_col].isin(ids_test)].reset_index(drop=True)\n",
    "X_test_df  = Final[ Final[id_col].isin(ids_test)].reset_index(drop=True)\n",
    "\n",
    "feat_cols = [c for c in Final.columns if c != id_col]\n",
    "\n",
    "X_train = X_train_df[feat_cols].astype(np.float32).values\n",
    "X_test  = X_test_df[feat_cols].astype(np.float32).values\n",
    "\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "scaler.fit(X_train)\n",
    "\n",
    "Z_train = scaler.transform(X_train).astype(np.float32)\n",
    "Z_test  = scaler.transform(X_test).astype(np.float32)\n",
    "\n",
    "train_embs = pd.DataFrame({\n",
    "    id_col: X_train_df[id_col].astype(np.int32).values,\n",
    "    \"embs\": list(Z_train),\n",
    "})\n",
    "test_embs = pd.DataFrame({\n",
    "    id_col: X_test_df[id_col].astype(np.int32).values,\n",
    "    \"embs\": list(Z_test),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f5c7924-ea5b-495a-91ba-79dca6b11a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_final = pd.concat((train_embs, test_embs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9dc3dd17-ff0a-4ceb-88aa-1ec69ab221de",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_final.to_csv(\n",
    "    'data/rosbank/embeddings/scaled_final.csv',\n",
    "    index=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
