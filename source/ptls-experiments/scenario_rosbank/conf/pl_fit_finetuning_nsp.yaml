defaults:
  - _self_
  - data_module_supervised/default@data_module
  - pl_module/defaults

seed_everything: 42
logger_name: nsp_finetuning
pretrained_encoder_path: models/nsp_model.p
fold_list:
  _target_: embeddings_validation.get_fold_list
  config_path: conf/embeddings_validation_baselines_supervised.yaml
fold_id: _

embedding_validation_results:
  model_name: nn
  feature_name: nsp_finetuning
  output_path: ${hydra:runtime.cwd}/results/nsp_finetuning_results.json

trainer:
  gpus: 1
  auto_select_gpus: false
  max_epochs: 10
  log_every_n_steps: 10
  enable_checkpointing: false
  deterministic: true

pl_module:
  seq_encoder:
    _target_: torch.load
    f: ${pretrained_encoder_path}
  pretrained_lr: 0.0001
  head:
    input_size: 512
    use_batch_norm: true
  lr_scheduler_partial:
    _partial_: true
    _target_: torch.optim.lr_scheduler.CosineAnnealingLR
    T_max: ${trainer.max_epochs}
    eta_min: 0
