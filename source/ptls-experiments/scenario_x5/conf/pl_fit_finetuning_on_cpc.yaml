defaults:
  - _self_
  - data_module_supervised/default@data_module
  - pl_module/defaults

seed_everything: 42
logger_name: cpc_finetuning
pretrained_encoder_path: models/cpc_model.p
fold_list:
  _target_: embeddings_validation.get_fold_list
  config_path: conf/embeddings_validation_baselines_supervised.yaml
fold_id: _

embedding_validation_results:
  model_name: nn
  feature_name: cpc_finetuning
  output_path: ${hydra:runtime.cwd}/results/cpc_finetuning_results.json

trainer:
  gpus: 1
  auto_select_gpus: false
  max_epochs: 8
  log_every_n_steps: 10
  enable_checkpointing: false
  deterministic: true

pl_module:
  seq_encoder:
    _target_: torch.load
    f: ${pretrained_encoder_path}
  pretrained_lr: 1e-05
  head:
    input_size: 128
    use_batch_norm: true
  optimizer_partial:
    _partial_: true
    _target_: torch.optim.Adam
    lr: 0.001
    weight_decay: 0.0
  lr_scheduler_partial:
    _partial_: true
    _target_: torch.optim.lr_scheduler.StepLR
    step_size: 1
    gamma: 0.6
